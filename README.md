# ğŸš€ **LLM MCQ Evaluator** - *Test Your LLMs with MCQ questions!* ğŸ§ ğŸ’¡  

![LLM MCQ Evaluator Logo](logo.png)

**A powerful Python module to evaluate Large Language Models (LLMs) using your own custom multiple-choice questions (MCQs).**  

ğŸ”¹ **Benchmark AI models** with ease  
ğŸ”¹ **Track accuracy & reasoning** in real-time  
ğŸ”¹ **Beautiful terminal UI** with progress tracking  
ğŸ”¹ **Switch LLM providers** in seconds  

---

## ğŸ›  **Installation in 2 Simple Steps!**  

### **1ï¸âƒ£ Clone & Install**  
```bash
git clone https://github.com/cyberytti/llm-mcq-evaluator.git
cd llm-mcq-evaluator  
```  

### **2ï¸âƒ£ Run & Evaluate!**  
```bash
uv run main.py  
```  

**Thatâ€™s it!** Your LLM evaluation starts instantly. ğŸ‰  

---

## ğŸ¯ **Why Use This Tool?**  

âœ… **Test any LLM** (Ollama, OpenAI, Anthropic, etc.)  
âœ… **Custom MCQ questions** (Math, Science, Coding, etc.)  
âœ… **Rich Terminal UI** with progress bars & live accuracy  
âœ… **Detailed logs** for deep analysis  

**Perfect for:**  
- Educators & students ğŸ“š  
- Developers testing LLMs ğŸ’»  

---

## ğŸš€ **Usage - Super Simple!**  

### **1ï¸âƒ£ Define Your MCQs** *(in `main.py`)*  
```python
from test_script import MCQEvaluator  

questions = [  
    "What is 2+2? (A) 3 (B) 4 (C) 5 (D) 6",  
    "Which of the following is a programming language? (A) Pythonâ€ƒ(B) HTMLâ€ƒ(C) JSONâ€ƒ(D) CSV"  
]  

answers = ["b", "a"]  # Correct options

# Run the test!  
evaluator = MCQEvaluator(model_name="deepseek-r1:8b-llama-distill-q8_0")

# Run evaluation with 30-second timeout per question 
evaluator.run_test(questions, answers, time_limit=30)  
```  

### **2ï¸âƒ£ See Instant Results!**  
ğŸ“Š **Live accuracy tracking**  
âœ… **Correct/Incorrect verdicts**  
ğŸ“ **Full logs in `/logs` folder**  

---

## ğŸ”„ **Switch LLM Providers in Seconds!**  

Just **modify 2 lines** in `test_script.py`:  
```python
from agno.models.ollama import Ollama  # ğŸ”„ Replace with your provider  
...  
model=Ollama(id=self.model_name),  # ğŸ”„ Update model initialization  
```  

**Supports:**  
- Ollama ğŸ¦™  
- OpenAI ğŸ¤–  
- Anthropic ğŸ§   
- Hugging Face ğŸ¤—  
- And more!  

---

## ğŸ“¹ **Watch It in Action!**

---

## ğŸ’¡ **Who Is This For?**  

âœ” **AI Engineers** â€“ Benchmark LLM performance  
âœ” **Researchers** â€“ Compare model reasoning  
âœ” **Educators** â€“ Test AI knowledge  
âœ” **Students** â€“ Learn how we can benchmark LLMs

---

## ğŸ“œ **License**  

MIT License - **Free to use & modify!**  

---

## ğŸ”¥ **Ready to Test Your LLM?**  

ğŸš€ **Clone the repo & start evaluating today!**  

```bash
git clone https://github.com/cyberytti/llm-mcq-evaluator.git
```  

**Happy Testing!** ğŸ¯ğŸ¤–
